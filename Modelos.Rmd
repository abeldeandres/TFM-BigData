---
title: "Modelos"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(Encoding="UTF-8")
```

##PRE-PROCESADO DE DATOS

###Búsqueda de OUTLIERS (datos anomálos)
```{r , echo=FALSE, include=FALSE}
library(normtest) ###REALIZA 5 PRUEBAS DE NORMALIDAD###
library(nortest) ###REALIZA 10 PRUEBAS DE NORMALIDAD###
library(moments) ###REALIZA 1 PRUEBA DE NORMALIDAD###
library(mvnTest)
library(dprep)
library(corrplot)
library(randomForest) #random forest
library(rpart)
library(rpart.plot)
library(devtools)
library(factoextra)
library(corrplot) #correlaciones
library(ggpubr) #correlaciones
library(reshape) #random forest
library(corpcor) #correlaciones parciales
library(MASS) #Stepwise
library(caret)

set.seed(42)
rm(list = ls())
final <- read.csv("C:/Users/Abel de Andrés Gómez/OneDrive/TFM/TraumaticData.csv",na.strings=c("","NA"))
```
* Se ha realizado un diagrama de caja para el conjunto de nuestro dataset
```{r , echo=FALSE}
boxplot(final,las = 2)
```
* Nos hemos dado cuenta que existen numerosos datos anomalos "Outliers" en la variable de edad.
```{r , echo=FALSE}
outliers<-boxplot(final$age,  ylab = "age")
```


* Estos son las edades anomalas:
```{r , echo=FALSE}
outliers<-outliers$out
outliers
```
  * Antes de proceder a eliminar los datos anomalos, vamos a ver la correlacion existente con la variable de *"Outcome"*:
```{r , echo=FALSE}
cor(final$age, final$outcome, method=c("pearson"))
```
  * Tambien vamos a observar como es la media y la mediana:
    + Media
    ```{r , echo=FALSE}
      mean(final$age)
    ```
    + Mediana
    ```{r , echo=FALSE}
      median(final$age)
    ```


  * Por tanto, hemos creado una funcion que nos elimine directamente los datos anomalos, es decir, todos aquellos datos que no se encuentren en el rango Q1-1.5·RIC o superiores a Q3+1.5·RIC se eliminaran. Siendo RIC el rango intercuartil (Q1-Q3)

```{r , echo=FALSE}
eliminar_outliers <- function(x, na.rm = TRUE) {
  qnt <- quantile(x, probs=c(.25, .75), na.rm = na.rm)
  H <- 1.5 * IQR(x, na.rm = na.rm)
  y <- x
  y[x < (qnt[1] - H)] <- NA
  y[x > (qnt[2] + H)] <- NA
  y
}

final$age= eliminar_outliers(final$age)
final <- final[!is.na(final$age),]
boxplot(final$age,  ylab = "age")
```

  * Ahora veamos la correlacion posterior a la eliminacion de los datos anomalos con la variable de *"Outcome"*;
```{r , echo=FALSE}
cor(final$age, final$outcome, method=c("pearson"))
```

  * Vemos que la correlacion ha empeorado un poco. De todas formas la correlacion entre la edad y el *"outcome"* es bastante debil.
  
  * Tambien vamos a observar como es la media y la mediana:
    + Media
    ```{r , echo=FALSE}
      mean(final$age)
    ```
    + Mediana
    ```{r , echo=FALSE}
      median(final$age)
    ```

  * Como se puede comprobar, la eliminación de los *outliers* no ha afectado demasiado a las variables estadisticos por lo que no existe motivo para su eliminación.
  
  * Por otro lado es necesario destacar que estos pacientes cuya edad es anomala (estadísticamente), en la naturaleza tampoco se consideran pacientes anomalos, ya que se encuentran en un rango de edades en las que sufrir un traumatismo craneocefálico es totalmente posible.

###ANÁLISIS DE NORMALIDAD

  * En primer lugar, visualizaremos la densidad de nuestras variables (individualmente), con el objetivo de observar a simple vista si cumplen o no con una distribucion normal.


```{r , echo=FALSE}
par(mfrow=c(2,2))
plot(density(final$sex))
plot(density(final$age))
plot(density(final$cause))
plot(density(final$ec))
par(mfrow=c(2,2))
plot(density(final$eye))
plot(density(final$motor))
plot(density(final$verbal))
plot(density(final$pupils))
par(mfrow=c(2,2))
plot(density(final$phm))
plot(density(final$sah))
plot(density(final$oblt))
plot(density(final$mdls))
par(mfrow=c(2,2))
plot(density(final$hmt))
plot(density(final$outcome))
```


  * Como podemos comprobar, al tratarse de variables discretas (excepto la variable de edad -*age*-), no lograremos conseguir una distribucion normal de forma individual.
  * Otro aspecto a tener en cuenta es que para que un conjunto de datos (teniendo en cuenta todas las variables) posea una distribucion normal, es necesario que todas las variables verifiquen normalidad univariante, ya que es una condicion necesaria (aunque no suficiente). Por lo tanto rechazamos la hipotesis de normalidad del conjunto de datos. 
  * Aún así, comprobaremos los resultados obtenidos mediante el Test de normalidad de Mardia:

```{r , echo=FALSE}

mardia(final)


```
  * Tambien vamos a utilizar el test de Henze-Zirkler:
```{r  , echo=FALSE}
h<-HZ.test(final, qqplot = TRUE)
```
```{r , echo=FALSE}
h
```
  * Como se puede comprobar, al ser el p-value menor de 0.05 en ambos test, los datos no se ajustan a una distribución normal.

###Estudio de correlación

* Para el estudio de la correlacion, utilizaremos el **coeficiente de correlacion de Pearson (R)**. Mediante el siguiente grafico, vamos a observar las relaciones que tienen los pares de variables entre si.

```{r , echo=FALSE}
MCOR <- cor(final)
#corrplot(MCOR, method = "number")
corrplot.mixed(MCOR) # Display the correlation coefficient
```


  * En este grafico podemos observar como por ejemplo las variables de *"motor"*, *"verbal"* y *"eye"* tienen bastante relacion y dependencia entre si. Sin embargo hay algo que no nos cuadra y es que no existe una gran dependencia entre la variable "age" y la variable de "outcome", aspecto que podria ser mas sustancial en la naturaleza.
  
  * Teniendo en cuenta los valores de la variable *"outcome"* (1 fallece y 0 vive), la correlacion negativa de las variable del test de glasgow (eye, motor, verbal) tiene sentido, puesto que en general, cuanto mayor sea el valor de estas variables, mejor pronostico de vida hay. La variable de "pupils" es al contrario, cuanto mayores sean sus valores, mas probable es el pronostico de fallecimiento.

```{r , echo=FALSE}
MCOR <- cor(final)
col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
cex.before <- par("cex")
par(cex = 0.5)
corrplot(MCOR, method="color", col=col(200),  
         type="upper", order="hclust", 
         addCoef.col = "black", # Add coefficient of correlation
         tl.col="black", tl.srt=45, tl.cex = 1/par("cex"), #Text label color and rotation
         # hide correlation coefficient on the principal diagonal
         diag=FALSE 
         )
```

* Como se ha podido apreciar en las 2 graficas anteriores, existe una gran correlacion entre las variables *"motor"*, *"eye"* y *"verbal"*. 

```{r , echo=FALSE}
ggscatter(final, x = "verbal", y = "eye", 
          add = "reg.line", conf.int = TRUE, 
          cor.coef = TRUE, cor.method = "pearson",
          xlab = "verbal", ylab = "eye")
```


* En la grafica anterior podemos volver a comprobar que existe una gran relacion lineal positiva entre las variables mas correlacionadas que son "verbal" y "eye".

* Matriz de Correlaciones Parciales
```{r , echo=FALSE}
matrizCorrelacionesParciales=cor2pcor(cor(final))
#corrplot.mixed(matrizCorrelacionesParciales, label=labels("hola")) 
colnames(matrizCorrelacionesParciales) <- c("sex", "age", "cause", "ec", "eye","motor", "verbal", "pupils", "phm","sah", "oblt", "mdls", "hmt", "outcome")
rownames(matrizCorrelacionesParciales) <- c("sex", "age", "cause", "ec", "eye","motor", "verbal", "pupils", "phm","sah", "oblt", "mdls", "hmt", "outcome")


corrplot.mixed(matrizCorrelacionesParciales) # Display the correlation
```
* Con la matriz de correlaciones parciales, obtendremos las correlaciones parciales que existe entre los pares de variables eliminando el efecto de las restantes. Vemos que las correlaciones fuertes se encuentran entre los mismos pares de variables que en la matriz de correlación total.

* A continuación, a modo de información, se muestra un listado en orden descendente con las mayores correlaciones existentes:

```{r , echo=FALSE}
variablesmascorreladas <- function(mydataframe,numtoreport)
  {
     # find the correlations
     cormatrix <- cor(mydataframe)
     # set the correlations on the diagonal or lower triangle to zero,
     # so they will not be reported as the highest ones:
     diag(cormatrix) <- 0
     cormatrix[lower.tri(cormatrix)] <- 0
     # flatten the matrix into a dataframe for easy sorting
     fm <- as.data.frame(as.table(cormatrix))
     # assign human-friendly names
     names(fm) <- c("First.Variable", "Second.Variable","Correlation")
     # sort and print the top n correlations
     head(fm[order(abs(fm$Correlation),decreasing=T),],n=numtoreport)
}
variablesmascorreladas(final,15)
```
* Las correlaciones entre las variables y la clase ordenadas en orden descendente son las siguientes:
```{r , echo=FALSE}
correlacionesconoutcome <- function(mydataframe,numtoreport)
  {
     # find the correlations
     cormatrix <- cor(mydataframe)
     # set the correlations on the diagonal or lower triangle to zero,
     # so they will not be reported as the highest ones:
     diag(cormatrix) <- 0
     cormatrix[lower.tri(cormatrix)] <- 0
     # flatten the matrix into a dataframe for easy sorting
     fm <- as.data.frame(as.table(cormatrix))
     # assign human-friendly names
     names(fm) <- c("First.Variable", "Second.Variable","Correlation")
     # sort and print the top n correlations
     fm<-fm[fm$Second.Variable=="outcome",]
     head(fm[order(abs(fm$Correlation),decreasing=T) ,],n=numtoreport)
}
correlacionesconoutcome(final,14)
```

* Por consiguiente consideramos que aunque exista una correlacion importante entre las variables "eye", "verbal" y "motor", no es lo suficientemente fuerte como para concluir que estas variables contienen la misma informacion y sea necesario la eliminacion de algunas de ellas. Por lo que no se procede a descartar ninguna de estas variables en estudios posteriores.

#Selección de variables con más importancia. 
## Uso de "Random Forest"

* En este apartado, se buscara obtener un listado con las variables mas importantes, usando el algoritmo de *"Random Forest"* para posteriormente tener en cuenta posibles descartes de variables en estudios posteriores.

* La idea que existe detras de los *"Random Forest"* es generar un numero importante de arboles, entrenarlos y calcular el promedio de su salida.

* En cada iteracion del algoritmo de *"Random Forest"* se genera un error conocido como **OOB**, este error ira aumentando o disminuyendo en cada iteracion y por cada variable que se incluya en el algoritmo.

* En cada paso (nodo) se recalcula el conjunto de **"m"** predictores permitidos. Lo más típico es elegir la raiz cuadrada del numero total de variables.En nuestro caso, contamos con un total de 13 variables, por lo que se escogerian 4 variables (redondeando hacia arriba en caso de no ser un número entero) en el caso de **árboles de clasificación** y **m=p/3** en el caso de **árboles de regresión**. Siendo **"p"** el numero de variables.

* Aun asi, es necesario calcular la variable **"mtry"**, puesto que es el único parámetro ajustable al cual los bosques aleatorios son algo sensibles. El "mtry" es el número de variables aleatorias utilizadas en cada árbol. La reduccion del "mtry" reduce tanto la correlación como la fuerza, aumentando ambas en caso contrario.

* En algún punto intermedio hay un rango "óptimo" de "mtry", generalmente bastante ancho.Usando la tasa de error de OOB, se puede encontrar rápidamente un valor de mínimo en el rango.

```{r , echo=FALSE, include=FALSE}
mtry=tuneRF(x = final,       # data set de entrenamiento 
       y = final$outcome,  # variable a predecir
       mtryStart  = 1,   # cantidad de variables inicial 
       stepFactor = 2,   # incremento de variables
       ntreeTry   = 500, # cantidad arboles a ejecutar en cada iteracion
       improve    = .01  # mejora minina del OOB para seguir iteraciones
      )
best.m <- mtry[mtry[, 2] == min(mtry[, 2]), 1]
```

```{r , echo=FALSE}
plot(mtry,type="o")
```

```{r , echo=FALSE}
print(mtry)
```

* Como se puede comprobar, el error OOB, se estabiliza, indicando cuantas particiones se deben realizar para obtener los mejores resultados En este caso, con **4 variables sería suficiente** (puesto que es el numero donde se estabiliza el error OOB).

* Las variables mas importantes utilizando el "mtry" son las siguientes:
```{r , echo=FALSE, include=FALSE}
fit=randomForest(outcome~.,data=final,importance=T,ntree=500, mtry=best.m)
#(VI_F=importance(fit))
#Creamos un objeto con las "importancias" de las variables
importancia=data.frame(importance(fit))
importancia<-sort_df(importancia,vars='X.IncMSE')
importancia
```
```{r , echo=FALSE}
#head(VI_F[order(abs(VI_F),decreasing=T), ],14)
importancia
```

* La variable **"IncNodePurity"** se la conoce tambien como la media de decrecimiento de de Gini. El indice de Gini es una "medida de desorden" en este caso *"IncNodePurity"* tiene el siguiente sentido, a mayor medida, mayor importancia en los modelos creados, puesto que valores proximos a 0 implican un mayor desorden. Por tanto, si computamos la media del "decrecimiento" del indice de Gini cuanto mayor sea esta medida, mas variabilidad aporta a la variable dependiente.

* Por otro lado, la variable **"IncMSE"** es la media de decrecimiento en la precision, y es tambien un indicador sobre la importancia de las variables en el modelo.

* El siguiente grafico representa la importancia de las variables segun su media y los valores de *"Random Forest"* mostrados anteriormente:

```{r , echo=FALSE}
varImpPlot(fit)
```


* A continuacion se va a utilizar un arbol de clasificación, que nos mostrara la importancia de las variables segun este algoritmo de clasificacion.

```{r , echo=FALSE}
fitpart=rpart(outcome ~.,
          data=final,parms = list(split = "gini"),maxdepth= 10)
rpart.plot(fitpart, type=4)
```

```{r , echo=FALSE}
fitpart
```

* Si recordamos, un resultado en el *"outcome"* de 0 eran aquellos pacientes que a los 6 meses habian vivido mientras quen un resultado de 1, significaba que los pacientes fallecian.Teniendo en cuenta este dato, podemos observar que los nodos del arbol son aquellas variables que el algoritmo considera mas relevantes y que hacen que un paciente viva o fallezca.

* La interpretacion que se da al arbol es la siguiente: Cada nodo contiene el porcentaje de informacion que contiene ademas de la media de la variable *"outcome"* en cada particion. Por ejemplo, la media de *"outcome"* es de 0.4, que coincide con el 0.4 del nodo raiz. Sin embargo, cuando la variable *"motor"* es mayor de 4.5, entonces el numero de datos se reduce al 64% y la media de *"outcome"* se vuelve a 0.26, significando para este caso que la mayoria de los pacientes viven, puesto que se aproxima a 0.

* Como conclusiones, utilizaremos las variables que se han considerado como mas importantes en el algoritmo del arbol de clasificacion y son las siguientes: *"motor"*,*"age"*, *"pupils"*,*"verbal"* y *"mdls"*.

## Uso del método de regresión paso a paso (Stepwise Regression)

* Este método es uno de los que se utilizan en la seleccion algoritmica del modelo. Se utiliza para identificar aquellas variables que se deberan integrar o no en los modelos a estudiar.

*La logica subyaciente de este algoritmo consiste en conservar las variables independientes que contienen informacion relevante y a la vez prescindir de aquellas que resulten redundantes respecto de las que quedaron en el modelo.

```{r , echo=FALSE}
full <- glm(outcome~.,family=binomial, data= final)
summary(full)
```
* Como podemos comprobar a simple vista, todas las variables son estadisticamente significante excepto "age" y "cause", cuyo p-valor es mayor a 0.05.

* A continuacion utilizamos el algoritmo de regresion paso a paso:

```{r , echo=FALSE}
step<-stepAIC(full,trace=FALSE)
step$anova
```
* Una vez mas podemos comprobar que las variables de *"cause"* y *"sex"* son las que se descartan usando este algoritmo.

* A continuación se va a utilizar la funcion train() del paquete caret que proporciona un flujo de trabajo sencillo para realizar selecciones paso a paso. Para esta prueba se ha utilizado la validacion cruzada con 10 iteraciones para estimar el error promedio de predicción (RMSE) de cada uno de los modelos. La métrica estadística RMSE se usa para comparar los modelos y elegir automáticamente el mejor, donde el mejor se define como el modelo que minimiza el RMSE.

* Se ha utilizado el parametro "nvmax" que corresponde al número máximo de predictores que se incorporarán en el modelo. Nosotros hemos utilizado todos.

* El parametro "nvmax" busca el mejor modelo de 1 variable, el mejor modelo de 2 variables, de 3, etc.

```{r , echo=FALSE, include=FALSE}
set.seed(42)
# Set up repeated k-fold cross-validation
train.control <- trainControl(method = "cv", number = 10)
# Train the model
step.model <- train(outcome ~., data = final,
                    method = "leapSeq", 
                    tuneGrid = data.frame(nvmax = 1:13),
                    trControl = train.control
                    )

```

* A continuacion el numero de variables que se utilizan para construir el mejor modelo.
```{r , echo=FALSE}
step.model$bestTune
```

* Por ultimo mostramos todos los modelos creados utilizando la funcion summary().
```{r , echo=FALSE}
summary(step.model$finalModel)
```
* Un asterisco especifica que la variable dada se incluye en el modelo correspondiente.

* Vemos que el mejor modelo contendria 11 variables y seria aquel que tendria en cuenta todos los predictores menos los predictores "sex" y "cause" como era de esperar.

#Analisis de PCA
* En primer lugar, antes de proceder con el analisis de componentes principales, vamos a tener en cuenta la matriz de correlaciones, puesto que un PCA tiene sentido si existen altas correlaciones entre las variables, ya que como se ha comentado con anteriorirdad, esto es indicativo de que existe informacion redundante y, por tanto, pocos factores explicaran gran parte de la variabilidad total.

* Como ya vimos con las matrices de correlaciones solo obtuvimos correlaciones medianamente fuertes entre las variables de *"motor"*, *"eye"* y *"verbal"*, pero la correlación no era significativa por lo que no se descartó ninguna variable.

* Un problema en el análisis de datos multivariante es la reducción de la dimensionalidad: es decir, si se puede conseguir  con precisión los valores de las variables (p)  con un pequeño subconjunto de ellas (r<p), habremos conseguido reducir la dimensión a costa de una pequeña perdida de información. 

* El análisis de componentes principales tiene este objetivo. Dada n observaciones de p variables, se analiza si es posible representar adecuadamente esta información con un conjunto menor de variables (construidas como combinaciones lineales de las originales).

* El primer paso en el análisis de componentes principales consiste en la obtención de los valores y vectores propios de la matriz de covarianzas muestral o de la matriz de coeficientes de correlación que se obtienen a partir de los datos. 

* Debemos saber que el analisis de componentes principales utiliza la versión normalizada de los predictores originales. Estas variables pueden encontrarse en distintas escalas (kilometros, litros, euros, etc.) y por lo tanto, las varianzas tambien tendran varias escalas.

* Realizar el PCA con variables no normalizadas dara lugar a que haya cargas bastante grandes para variables con una varianza alta y a su vez, esto llevará a la dependencia de una componente principal con la variable con la varianza mas alta. Esto no es deseable. Por lo que se llevara a cabo una normalización de las variables. Al normalizar las variables, la distribución de la variabilidad entre las componentes parece más racional.

```{r echo=FALSE, , echo=FALSE}
#Max-Min Normalization
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

finalNorm <- as.data.frame(lapply(final, normalize))
```

* Veamos que ocurre si utilizamos la **matriz de covarianza**, sin haber normalizado las variables:
```{r echo=FALSE, , echo=FALSE}
pca.final<- prcomp(final,scale=FALSE)
summary(pca.final)
plot(pca.final)
```

```{r echo=FALSE, , echo=FALSE}
biplot(pca.final, scale = 0)
```

* Como se puede comprobar en la grafica anterior, al no haber escalado las variables, la primera componente principal (PC1) esta dominada por la variable *"age"*, mientras que la segunda componente principal esta dominada por las variables: *"eye"*, *"motor"* y *"verbal"*.

* Ahora vamos a utilizar la **matriz de covarianza**, habiendo normalizado todas las variables.
```{r echo=FALSE, , echo=FALSE}
pca.final <- prcomp(finalNorm)
summary(pca.final)
```
```{r echo=FALSE, , echo=FALSE}
plot(pca.final)
```

```{r echo=FALSE, , echo=FALSE}
biplot(pca.final, scale = 0)
```

* Como se puede comprobar en la grafica anterior, al normalizar las variables, vemos que el peso de estas se distribuye de forma mas uniforme entre las 2 componentes principales.

  * Para elegir nuestras componentes principales, podremos utilizar dos métodos: 
    + Por un lado, podemos utilizar el **criterio de Kaiser**, que consiste en conservar aquellos factores cuya desviación estándar al cuadrado asociada sea mayor que 1. 
    
    ```{r echo=FALSE, , echo=FALSE}
        (pca.final$sdev)^2 
    ```
      Como se puede comprobar, utilizando este criterio, podriamos quedarnos con los componentes PC1,PC2,PC3,PC4 y PC5.
    + Otra forma para saber cuántos componentes tener en cuenta es mantener el número decomponentes necesarios para explicar al menos un porcentaje del total de la varianza. Por ejemplo, es importante **explicar al menos un 80%** de la varianza.
    
    ```{r echo=FALSE, , echo=FALSE}
      get_eig(pca.final)
    ```
    ```{r echo=FALSE, , echo=FALSE}
      fviz_eig(pca.final,ncp = 14)
    ```

* Segun este criterio, deberiamos quedarnos con los primeros componentes principales: PC1,PC2,PC3,PC4,PC5,PC6,PC7,PC8 y PC9.
 
* A continuacion, podremos ver la carga de cada variable respecto a las componentes principales.

```{r echo=FALSE, , echo=FALSE}
pca.final$rotation
```

  * Como conclusiones teniendo en cuenta el PCA y las matrices de correlaciones, no se puede descartar ninguna variable por los siguientes motivos:
    + Las correlaciones entre las variables *"eye"*, *"motor"* y *"verbal"* no son lo suficientemente fuertes como para considerar que existe informacion redundante.El resto de pares de variables tienen una correlacion poco significativa.
    + Los criterios utilizados para elegir las componentes principales nos han indicado que se necesitan al menos 5 componentes principales usando el criterio de Kaiser y 9 utilizando el criterio del 80% de la proporcion de la varianza. Teniendo en cuenta que poseemos 14 variables, la reducción no es significativa y se perderia interpretabilidad.