---
title: "Modelos"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(Encoding="UTF-8")
```

##USO DE MODELOS
###Búsqueda de OUTLIERS (datos anomálos)
```{r , echo=FALSE, include=FALSE}
library(normtest) ###REALIZA 5 PRUEBAS DE NORMALIDAD###
library(nortest) ###REALIZA 10 PRUEBAS DE NORMALIDAD###
library(moments) ###REALIZA 1 PRUEBA DE NORMALIDAD###
library(mvnTest)
library(dprep)
library(corrplot)
library(randomForest) #random forest
library(rpart)
library(rpart.plot)
library(devtools)
library(factoextra)
library(corrplot) #correlaciones
library(ggpubr) #correlaciones
library(reshape) #random forest
library(corpcor) #correlaciones parciales

rm(list = ls())
final <- read.csv("C:/Users/Abel de Andrés Gómez/OneDrive/TFM/TraumaticData.csv",na.strings=c("","NA"))
```
* Se ha realizado un diagrama de caja para el conjunto de nuestro dataset
```{r , echo=FALSE}
boxplot(final)
```
* Nos hemos dado cuenta que existen numerosos datos anomalos "Outliers" en la variable de edad.
```{r , echo=FALSE}
outliers<-boxplot(final$age,  ylab = "age")
```


* Estos son las edades anomalas:
```{r , echo=FALSE}
outliers<-outliers$out
outliers
```
* Antes de proceder a eliminar los datos anomalos, vamos a ver la correlacion existente con la variable de "Outcome";
```{r , echo=FALSE}
cor(final$age, final$outcome, method=c("pearson"))
```
* Tambien vamos a observar como es la media y la mediana:
* Media
```{r , echo=FALSE}
mean(final$age)
```
* Mediana
```{r , echo=FALSE}
median(final$age)
```


* Por tanto, hemos creado una funcion que nos elimine directamente los datos anomalos, es decir, todos aquellos datos que no se encuentren en el rango Q1-1.5·RIC o superiores a Q3+1.5·RIC se eliminaran. Siendo RIC el rango intercuartil (Q1-Q3)

```{r , echo=FALSE}
eliminar_outliers <- function(x, na.rm = TRUE) {
  qnt <- quantile(x, probs=c(.25, .75), na.rm = na.rm)
  H <- 1.5 * IQR(x, na.rm = na.rm)
  y <- x
  y[x < (qnt[1] - H)] <- NA
  y[x > (qnt[2] + H)] <- NA
  y
}

final$age= eliminar_outliers(final$age)
final <- final[!is.na(final$age),]
boxplot(final$age,  ylab = "age")
```

* Ahora veamos la correlacion posterior a la eliminacion de los datos anomalos con la variable de "Outcome";
```{r , echo=FALSE}
cor(final$age, final$outcome, method=c("pearson"))
```

* Vemos que la correlacion ha empeorado un poco. De todas formas la correlacion entre la edad y el "outcome" es bastante debil.

* Tambien vamos a observar como es la media y la mediana:
* Media
```{r , echo=FALSE}
mean(final$age)
```
* Mediana
```{r , echo=FALSE}
median(final$age)
```

* Como se puede comprobar, la eliminacion de los outliers no ha afectado demasiado a las variables estadisticos.

###ANÁLISIS DE NORMALIDAD
* En primer lugar, visualizaremos la densidad de nuestras variables (individualmente), con el objetivo de observar a simple vista si cumplen o no con una distribucion normal.


```{r , echo=FALSE}
par(mfrow=c(2,2))
plot(density(final$sex))
plot(density(final$age))
plot(density(final$cause))
plot(density(final$ec))
par(mfrow=c(2,2))
plot(density(final$eye))
plot(density(final$motor))
plot(density(final$verbal))
plot(density(final$pupils))
par(mfrow=c(2,2))
plot(density(final$phm))
plot(density(final$sah))
plot(density(final$oblt))
plot(density(final$mdls))
par(mfrow=c(2,2))
plot(density(final$hmt))
plot(density(final$outcome))
```


* Como podemos comprobar, al tratarse de variables discretas (excepto la variable de edad -age-), no lograremos conseguir una distribucion normal de forma individual.
* Otro aspecto a tener en cuenta es que para que un conjunto de datos (teniendo en cuenta todas las variables) posea una distribucion normal, es necesario que todas las variables verifiquen normalidad univariante, ya que es una condicion necesaria (aunque no suficiente). Por lo tanto rechazamos la hipotesis de normalidad del conjunto de datos. 
* Aún así, comprobaremos los resultados obtenidos mediante el Test de normalidad de Mardia:

```{r eval=FALSE, , echo=FALSE, include=FALSE}

mardia(final)


```
* Tambien vamos a utilizar el test de Henze-Zirkler:
```{r eval=FALSE, , echo=FALSE, include=FALSE}
h<-HZ.test(final, qqplot = TRUE)
```
```{r eval=FALSE, , echo=FALSE, include=FALSE}
h
```
* Como se puede comprobar, al ser el p-value menor de 0.05 en ambos test, los datos no se ajustan a una distribución normal.

###Estudio de correlación
* Para el estudio de la correlacion, utilizaremos el coeficiente de correlacion de Pearson (R). Mediante el siguiente grafico, vamos a observar las relaciones que tienen los pares de variables entre si.

```{r , echo=FALSE}
MCOR <- cor(final)
#corrplot(MCOR, method = "number")
corrplot.mixed(MCOR) # Display the correlation coefficient
```


* En este grafico podemos observar como por ejemplo las variables de "motor", "verbal" y "eye" tienen bastante relacion y dependencia entre si. Sin embargo hay algo que no nos cuadra y es que no existe una gran dependencia entre la variable "age" y la variable de "outcome", aspecto que podria ser mas sustancial en la naturaleza. 
* Teniendo en cuenta los valores de la variable "outcome" (1 fallece y 0 vive), la correlacion negativa de las variable del test de glasgow (eye, motor, verbal) tiene sentido, puesto que en general, cuanto mayor sea el valor de estas variables, mejor pronostico de vida hay. La variable de "pupils" es al contrario, cuanto mayores sean sus valores, mas probable es el pronostico de fallecimiento.

```{r , echo=FALSE}
MCOR <- cor(final)
col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
cex.before <- par("cex")
par(cex = 0.5)
corrplot(MCOR, method="color", col=col(200),  
         type="upper", order="hclust", 
         addCoef.col = "black", # Add coefficient of correlation
         tl.col="black", tl.srt=45, tl.cex = 1/par("cex"), #Text label color and rotation
         # hide correlation coefficient on the principal diagonal
         diag=FALSE 
         )
```

* Como se ha podido apreciar en las 2 graficas anteriores, existe una gran correlacion entre las variables "motor", "eye" y "verbal". 

```{r , echo=FALSE}
ggscatter(final, x = "verbal", y = "eye", 
          add = "reg.line", conf.int = TRUE, 
          cor.coef = TRUE, cor.method = "pearson",
          xlab = "verbal", ylab = "eye")
```


* En la grafica anterior podemos volver a comprobar que existe una gran relacion lineal positiva entre las variables mas correlacionadas que son "verbal" y "eye".

* Matriz de Correlaciones Parciales
```{r , echo=FALSE}
matrizCorrelacionesParciales=cor2pcor(cor(final))
#corrplot.mixed(matrizCorrelacionesParciales, label=labels("hola")) 
colnames(matrizCorrelacionesParciales) <- c("sex", "age", "cause", "ec", "eye","motor", "verbal", "pupils", "phm","sah", "oblt", "mdls", "hmt", "outcome")
rownames(matrizCorrelacionesParciales) <- c("sex", "age", "cause", "ec", "eye","motor", "verbal", "pupils", "phm","sah", "oblt", "mdls", "hmt", "outcome")


corrplot.mixed(matrizCorrelacionesParciales) # Display the correlation
```
* Con la matriz de correlaciones parciales, obtendremos las correlaciones parciales que existe entre los pares de variables eliminando el efecto de las restantes. Vemos que las correlaciones fuertes se encuentran entre los mismos pares de variables que en la matriz de correlación total.

* A continuación, a modo de información, se muestra un listado en orden descendente con las mayores correlaciones existentes:

```{r , echo=FALSE}
variablesmascorreladas <- function(mydataframe,numtoreport)
  {
     # find the correlations
     cormatrix <- cor(mydataframe)
     # set the correlations on the diagonal or lower triangle to zero,
     # so they will not be reported as the highest ones:
     diag(cormatrix) <- 0
     cormatrix[lower.tri(cormatrix)] <- 0
     # flatten the matrix into a dataframe for easy sorting
     fm <- as.data.frame(as.table(cormatrix))
     # assign human-friendly names
     names(fm) <- c("First.Variable", "Second.Variable","Correlation")
     # sort and print the top n correlations
     head(fm[order(abs(fm$Correlation),decreasing=T),],n=numtoreport)
}
variablesmascorreladas(final,15)
```
* Las correlaciones entre las variables y la clase ordenadas en orden descendente son las siguientes:
```{r , echo=FALSE}
correlacionesconoutcome <- function(mydataframe,numtoreport)
  {
     # find the correlations
     cormatrix <- cor(mydataframe)
     # set the correlations on the diagonal or lower triangle to zero,
     # so they will not be reported as the highest ones:
     diag(cormatrix) <- 0
     cormatrix[lower.tri(cormatrix)] <- 0
     # flatten the matrix into a dataframe for easy sorting
     fm <- as.data.frame(as.table(cormatrix))
     # assign human-friendly names
     names(fm) <- c("First.Variable", "Second.Variable","Correlation")
     # sort and print the top n correlations
     fm<-fm[fm$Second.Variable=="outcome",]
     head(fm[order(abs(fm$Correlation),decreasing=T) ,],n=numtoreport)
}
correlacionesconoutcome(final,14)
```

* Por consiguiente consideramos que aunque exista una correlacion importante entre las variables "eye", "verbal" y "motor", no es lo suficientemente fuerte como para concluir que estas variables contienen la misma informacion y sea necesario la eliminacion de algunas de ellas. Por lo que no se procede a descartar ninguna de estas variables en estudios posteriores.

#Selección de variables con más importancia. Uso de Random Forest
* En este apartado, se buscara obtener un listado con las variables mas importantes, usando el algoritmo de Random Forest para posteriormente coger las variables mas importantes, que son las siguientes:
```{r , echo=FALSE, include=FALSE}
fit=randomForest(outcome~.,data=final,importance=T)
#(VI_F=importance(fit))
#Creamos un objeto con las "importancias" de las variables
importancia=data.frame(importance(fit))
importancia<-sort_df(importancia,vars='IncNodePurity')
importancia

```
```{r , echo=FALSE, include=FALSE}


```
```{r , echo=FALSE}
#head(VI_F[order(abs(VI_F),decreasing=T), ],14)
importancia
```
* La variable IncNodePurity se la conoce tambien como la media de decrecimiento de de Gini.El indice de Gini es una "medida de desorden" en este caso "IncNodePurity" tiene el siguiente sentido, a mayor medida, mayor importancia en los modelos creados, puesto que valores proximos a 0 implican un mayor desorden. Por tanto, si computamos la media del "decrecimiento" del indice de Gini cuanto mayor sea esta medida, mas variabilidad aporta a la variable dependiente.

* Por otro lado, la variable IncMSE es la media de decrecimiento en la precision, y es tambien un indicador sobre la importancia de las variables en el modelo.

* El siguiente grafico representa la importancia de las variables segun su media y los valores de Random Forest mostrados anteriormente:
```{r , echo=FALSE}
varImpPlot(fit)
```

* En general, se suele utilizar la raiz cuadrada del numero total de variables presentes en los datos. En nuestro caso, contamos con un total de 13 variables, por lo que se escogerian 4 variables: "age", "motor","verbal" y "eye"


* A continuacion se va a utilizar un arbol de clasificación, que nos mostrara la importancia de las variables segun este algoritmo de clasificacion.

```{r , echo=FALSE}
fitpart=rpart(outcome ~.,
          data=final,parms = list(split = "gini"),maxdepth= 10)
rpart.plot(fitpart, type=4)
```

```{r , echo=FALSE}
fitpart
```

* Si recordamos, un resultado en el "outcome" de 0 eran aquellos pacientes que a los 6 meses habian vivido mientras quen un resultado de 1, significaba que los pacientes fallecian.Teniendo en cuenta este dato, podemos observar que los nodos del arbol son aquellas variables que el algoritmo considera mas relevantes y que hacen que un paciente viva o fallezca.

* La interpretacion que se da al arbol es la siguiente: Cada nodo contiene el porcentaje de informacion que contiene ademas de la media de la variable "outcome" en cada particion. Por ejemplo, la media de "outcome" es de 0.4, que coincide con el 0.4 del nodo raiz. Sin embargo, cuando la variable "motor" es mayor de 4.5, entonces el numero de datos se reduce al 64% y la media de "outcome" se vuelve a 0.26, significando para este caso que la mayoria de los pacientes viven, puesto que se aproxima a 0.

* Como conclusiones, utilizaremos las variables que se han considerado como mas importantes en el algoritmo del arbol de clasificacion y son las siguientes: "motor","age", "pupils","verbal" y "mdls".

#Analisis de PCA
* En primer lugar, antes de proceder con el analisis de componentes principales, vamos a tener en cuenta la matriz de correlaciones, puesto que un PCA tiene sentido si existen altas correlaciones entre las variables, ya que como se ha comentado con anteriorirdad, esto es indicativo de que existe informacion redundante y, por tanto, pocos factores explicaran gran parte de la variabilidad total.

* Como ya vimos con las matrices de correlaciones solo obtuvimos correlaciones medianamente fuertes entre las variables de "motor", "eye" y "verbal", pero la correlacion no era significativa por lo que no se descarto ninguna variable.

* Un problema en el análisis de datos multivariante es la reducción de la dimensionalidad: es decir, si se puede conseguir  con precisión los valores de las variables (p)  con un pequeño subconjunto de ellas (r<p), habremos conseguido reducir la dimensión a costa de una pequeña perdida de información. 
* El análisis de componentes principales tiene este objetivo. Dada n observaciones de p variables, se analiza si es posible representar adecuadamente esta información con un conjunto menor de variables (construidas como combinaciones lineales de las originales).  
* El primer paso en el análisis de componentes principales consiste en la obtención de los valores y vectores propios de la matriz de covarianzas muestral o de la matriz de coeficientes de correlación que se obtienen a partir de los datos. En nuestro caso vamos a utilizar la matriz de correlaciones. 
```{r echo=FALSE, , echo=FALSE}
pca.final <- prcomp(final,scale=T)
summary(pca.final)
```

* Para elegir nuestras componentes principales, podremos utilizar dos métodos: 
* Por un lado, podemos utilizar el criterio de Kaiser, que consiste en conservar aquellos factores cuya desviación estándar al cuadrado asociada sea mayor que 1. 
```{r echo=FALSE, , echo=FALSE}
(pca.final$sdev)^2 
```
* Como se puede comprobar, utilizando este criterio, podriamos quedarnos con los componentes PC1,PC2,PC3,PC4 y PC5.
* Otra forma para saber cuántos componentes tener en cuenta es mantener el número decomponentes necesarios para explicar al menos un porcentaje del total de la varianza. Por ejemplo, es importante explicar al menos un 80% de la varianza.
```{r echo=FALSE, , echo=FALSE}
get_eig(pca.final)
```
```{r echo=FALSE, , echo=FALSE}
fviz_eig(pca.final,ncp = 14)
```

* Segun este criterio, deberiamos quedarnos con los primeros componentes principales: PC1,PC2,PC3,PC4,PC5,PC6,PC7,PC8 y PC9.
 
* A continuacion, podremos ver la carga de cada variable respecto a las componentes principales.
```{r echo=FALSE, , echo=FALSE}
pca.final$rotation
```

  * Como conclusiones teniendo en cuenta el PCA y las matrices de correlaciones, no se puede descartar ninguna variable por los siguientes motivos:
    + Las correlaciones entre las variables "eye", "motor" y "verbal" no son lo suficientemente fuertes como para considerar que existe informacion redundante.El resto de pares de variables tienen una correlacion poco significativa.
    + Los criterios utilizados para elegir las componentes principales nos han indicado que se necesitan al menos 5 componentes Principales usando el criterio de Kaiser y 9 utilizando el criterio del 80% de la proporcion de la varianza. Teniendo en cuenta que poseemos 14 variables, la reducción no es significativa. 