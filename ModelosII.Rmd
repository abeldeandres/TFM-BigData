---
title: "ModelosII"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(Encoding="UTF-8")

```

```{r , echo=FALSE, include=FALSE}
library(caTools)
library(caret)
library(randomForest)
library(nnet)
library(neuralnet)
library(party) #ctree
library(gbm)
library(neuralnet)
library(rfUtilities)

rm(list = ls())
final <- read.csv("C:/Users/Abel de Andrés Gómez/OneDrive/TFM/TraumaticData.csv",na.strings=c("","NA"))


```
## TERMINOS GENERALES
### Matriz de confusión
  * Una matriz de confusión es una tabla que se usa a menudo para describir el rendimiento de un modelo de clasificación sobre en un conjunto de datos de prueba para los cuales se conocen los valores verdaderos.

  * Definamos ahora los términos más básicos para nuestro estudio:
    + Verdaderos Positivos (TP): estos son casos en los que predijimos que sí (van a fallecer), y sí fallecen.
    + Verdaderos Negativos (TN): Predijimos que no iban a fallecer, y los pacientes no fallecen.
    + Falsos Positivos (FP): Predijimos que sí iban a fallecer, pero en realidad no han fallecido.
    + Falsos Negativos (FN): Predijimos que no, pero en realidad tienen la enfermedad.
    
  * En nuestro estudio se ha tomado como positivo, el valor de "1", es decir, que el paciente fallezca.
  
  * Otra información a tener en cuenta en los siguientes graficos:
    + Reference 0, Prediction 0 -> Verdaderos Negativos (TN).
    + Reference 1, Prediction 0 -> Falsos Negativos (FN)
    + Reference 0, Prediction 1 -> Falsos Positivos (FP)
    + Reference 1, Prediction 1 -> Verdaderos Positivos (TP)
    
## Datos de entrenamiento y test
* En primer lugar, diviremos los datos en un conjunto de entreno (train) y un conjunto de pruebas (test). Tenemos 6930 registros por lo que el 30% son datos de prueba (2096) y el 70% datos de entrenamiento del modelo (4890).


```{r , echo=FALSE}
#Cambiamos la variable outcome a factor para trabajar con el modelo
final$outcome<-as.factor(final$outcome)
set.seed(42)
ind<-sample.split(Y=final$outcome,SplitRatio =0.7)
train <- final[ind,]
test <- final[!ind,]

```

## REGRESION LOGISTICA 
### Usando todos los predictores
* En primer lugar vamos a construir nuestro modelo de regresion logistica con los datos de entrenamiento.
```{r , echo=FALSE}
set.seed(42)
modelo.logit <- glm(outcome ~ ., 
                    data = train, family = "binomial")
summary(modelo.logit)
```

* Una vez hecho esto, vamos a utilizar los datos de test para predecir el modelo que hemos construido y comprobaremos el ajuste de este.
```{r , echo=FALSE}
options(warn=-1) 
pred <- predict(modelo.logit, newdata = test, type = "response")  # predicted probabilities
options(warn=1) 

out_pred_num <- ifelse(pred > 0.5, 1, 0)
out_pred <- factor(out_pred_num, levels=c(0, 1))

confusionMatrix<-confusionMatrix(data = out_pred, reference = test$outcome,positive="1")
confusionMatrix

```

```{r , echo=FALSE}
fourfoldplot(confusionMatrix$table)
```

* Como podemos comprobar, el modelo se ajusta bastante bien, con un 76%. Dicho de otra forma, el modelo es capaz de predecir correctamente un 76% de los datos de los pacientes.

### Eliminando los predictores "sex" y "cause""
* Al igual que hicimos en el modelo anterior, vamos a construir el modelo eliminando las variables de "sex" y "cause"
```{r , echo=FALSE}
set.seed(42)
modelo.logit2 <- glm(outcome ~ age+ec+eye+motor+verbal+pupils+phm+sah+oblt+mdls+hmt, 
                    data = train, family = "binomial")
summary(modelo.logit2)
```

* Una vez hecho esto, vamos a utilizar los datos de test para predecir el modelo que hemos construido y comprobaremos el ajuste de este.
```{r , echo=FALSE}
options(warn=-1) 
pred2 <- predict(modelo.logit2, newdata = test, type = "response")  # predicted probabilities
options(warn=1) 

out_pred_num2 <- ifelse(pred2 > 0.5, 1, 0)
out_pred2 <- factor(out_pred_num2, levels=c(0, 1))

confusionMatrix2<-confusionMatrix(data = out_pred2, reference = test$outcome,positive="1")
confusionMatrix2

```

```{r , echo=FALSE}
fourfoldplot(confusionMatrix2$table)
```

* Como podemos comprobar, aunque el AIC ha mejorado (reducido su valor) utilizando este modelo (sin las variables de sexo y causa) respecto al modelo con todas las variables, ha empeorado las prediciones.


## RANDOM FOREST u otros modelos de arboles (GBM, Adaboost, XGBM, etc.)
* Random forest es otra técnica de aprendizaje automático y nace como mejora sustancial de los árboles simples. Combina una cantidad grande de árboles de decisión independientes probados sobre conjuntos de datos aleatorios con igual distribución.

  * La fase de aprendizaje consiste en crear una gran cantidad de árboles de decisión independientes. Estos arboles se construyen a partir de los datos de entrada ligeramente modificados. Se modifica el conjunto inicial de partida, de la siguiente forma:

    + Se selecciona aleatoriamente con reemplazamiento un porcentaje de datos de la muestra total.

  * Es habitual incluir un segundo nivel aleatoriedad, esta vez afectando los atributos:

    + En cada nodo, al seleccionar la partición óptima, tenemos en cuenta sólo una porción de los atributos, elegidos al azar en cada ocasión.

  * Una vez que se tienen muchos árboles -500 por ejemplo- la fase de clasificación se lleva a cabo de la siguiente manera:

    + Cada árbol se evalúa de forma independiente y la predicción del bosque será la media de los 500 árboles. La proporción de árboles que toman una misma respuesta se interpreta como la probabilidad de la misma.

* El modelo de Random Forest tambien lo construiremos con los mismos datos de entrenamiento que utilizamos en la regresion logistica.

```{r , echo=FALSE}
set.seed(42)
model.RF=randomForest(outcome~.,data=train,ntree=500)
print(model.RF)
```

```{r , echo=FALSE}
train.RF<-train
train.RF$outcome<-as.numeric(train.RF$outcome)
mtry=tuneRF(x = train.RF,       # data set de entrenamiento 
       y = train.RF$outcome,  # variable a predecir
       mtryStart  = 1,   # cantidad de variables inicial 
       stepFactor = 2,   # incremento de variables
       ntreeTry   = 500, # cantidad arboles a ejecutar en cada iteracion
       improve    = .01  # mejora minina del OOB para seguir iteraciones
      )
best.m <- mtry[mtry[, 2] == min(mtry[, 2]), 1]
print(mtry)
```

```{r , echo=FALSE}
print(best.m)
```
* Una vez obtenido el optimo valor de mtry, vamos a volver a construir el modelo con el mtry optimo.
```{r , echo=FALSE}
model.RF=randomForest(outcome~.,data=final,importance=T,ntree=500, mtry=best.m)
print(model.RF)
```

* Despues de construir el modelo, vamos a utilizar los datos de test para predecirlo. Comprobaremos una vez mas el ajuste de este.

```{r , echo=FALSE}
predRF <- predict(model.RF, newdata = test)
confusionMatrixRF<-confusionMatrix(data=predRF,reference=test$outcome,positive="1")
confusionMatrixRF
```

```{r , echo=FALSE}
fourfoldplot(confusionMatrixRF$table)
```


## Arbol de decisión
```{r , echo=FALSE}
set.seed(42)
train.ctree=ctree(outcome ~ age+ec+eye+motor+verbal+pupils+phm+sah+oblt+mdls+hmt+sex+cause, data=train)
#train.ctree
#plot(train.ctree, main="Árbol de decisión")
```
```{r , echo=FALSE}
ctree.predict=predict(train.ctree,test)
confusionMatrixCTREE<-confusionMatrix(data=ctree.predict,reference=test$outcome,positive="1")
confusionMatrixCTREE
```

```{r , echo=FALSE}
fourfoldplot(confusionMatrixCTREE$table)
```

## GBM
* La idea general es obtener una secuencia de árboles (muy) simples, donde cada árbol sucesivo se construye con los residuos de predicción del árbol anterior.

* Una vez generado los arboles uno a uno, se suman las predicciones de los árboles individuales:

* D(x)= dtree1(x) + dtree2(x) + ...

*  El siguiente árbol de decisiones -dtree3- intenta reducir la diferencia entre la función objetivo f(x) y la predicción del conjunto actual al reconstruir el residuo (dtree1 +dtree2).

* Ademas, el siguiente árbol -dtree3- en el conjunto debe complementarse bien con los árboles existentes y minimizar el error de entrenamiento del conjunto.

* D(x) + dtree3(x) = f(x)

* Para acercarnos a una prediccion sin errores, entrenamos un árbol para reconstruir la diferencia entre la función objetivo y las predicciones actuales de un conjunto, esta diferencia se denomina residuo: 

* R(x)= f(x) - D(x)

* Como podemos observar, si el árbol de decisión reconstruye completamente R(x), todo el conjunto daria predicciones sin errores, es decir, predicciones exactas. Esto en la practica nunca sucede.

* Uno de los principales problemas de todos los algoritmos de aprendizaje automático es 'saber cuándo detenerse', es decir, cómo evitar que el algoritmo de aprendizaje se ajuste tanto, que probablemente no mejore la validez predictiva del modelo. Este problema también se conoce como el problema del sobreajuste (overfitting).

* Para establecer el limite de estos algoritmos, tenemos en cuenta la iteracion (numero de arboles) en la que se consigue un menor error.


* El modelo de GBM tambien lo construiremos con los mismos datos de entrenamiento que utilizamos en los modelos anteriores.

```{r , echo=FALSE}
#https://rpubs.com/omicsdata/gbm
train$outcome<-as.numeric(train$outcome)
train = transform(train, outcome=outcome-1)
set.seed(42)
model.gbm = gbm(outcome ~ ., data = train, shrinkage=0.01, distribution = 'bernoulli', cv.folds=5, n.trees=3000, verbose=F)
model.gbm
```
* A continuacion mostramos un grafico con la mejor iteracion para evitar el conocido sobreajuste (**overfitting**)
* Utilizando OOB
```{r , echo=FALSE}

best.iter.oob <- gbm.perf(model.gbm,method="OOB")  # returns out-of-bag estimated best number of trees
print(best.iter.oob)

```
* Utilizando Cross validation
```{r , echo=FALSE}

best.iter.cv <- gbm.perf(model.gbm,method="cv")   # returns 5-fold cv estimate of best number of trees
print(best.iter.cv)


```

* Hemos utilizado OOB ya que el numero de iteraciones es menor.

* Teniendo en cuenta el numero optimo de iteraciones, volvemos a crear el modelo con este numero:
```{r , echo=FALSE}
set.seed(42)
model.gbm = gbm(outcome ~ ., data = train, shrinkage=0.01, distribution = 'bernoulli', cv.folds=5, n.trees=best.iter.oob, verbose=F)
model.gbm
```

* El modelo tambien nos indica la importancia de los predictores:
```{r , echo=FALSE}
summary(model.gbm)
```

* Una vez mas, podemos comprobar que las variables "age" esta en la cabeza de las variables mas influyentes en el modelo y por el contrario la variable "sex", la que menos.


```{r , echo=FALSE}
model.predictGBM = predict(model.gbm, test, type="response")
out_pred_numGBM <- ifelse(model.predictGBM > 0.5, 1, 0)
out_predGBM <- factor(out_pred_numGBM, levels=c(0, 1))
confusionMatrixGBM<-confusionMatrix(data = out_predGBM, reference = test$outcome,positive="1")
confusionMatrixGBM
```
```{r , echo=FALSE}
fourfoldplot(confusionMatrixGBM$table)
```
```{r , echo=FALSE}
#Cambiamos el outcome a factor para futuras operaciones
train$outcome<-as.factor(train$outcome)
```
* Como conclusiones, utilizando OOB, hemos obtenido una mejor precision. Con Cross Validation obtuvimos un 0.7676527% de precision mientras que con OOB obtuvimos 0.7719%.


## Redes de neuronas

```{r , echo=FALSE}
set.seed(42)
model.nn = nnet(outcome ~ ., data = train, size = 2, rang = 0.1, maxit = 200)
print(model.nn)
```

```{r , echo=FALSE, include=FALSE}
#train$outcome<-as.numeric(train$outcome)
#train = transform(train, outcome=outcome-1)
#nn <- neuralnet(outcome ~ age+ec+eye+motor+verbal+pupils+phm+sah+oblt+mdls+hmt+sex+cause, data=train)
#nn$result.matrix
```

```{r , echo=FALSE,include=FALSE}
#plot(nn)
```

```{r , echo=FALSE}
model.predictNET = predict(model.nn, test)
#model.predict<-as.factor(model.predict)
out_pred_numNET <- ifelse(model.predictNET > 0.5, 1, 0)
out_predNET <- factor(out_pred_numNET, levels=c(0, 1))
confusionMatrixNET<-confusionMatrix(data=out_predNET,reference=test$outcome,positive="1")
confusionMatrixNET
```

```{r , echo=FALSE}
fourfoldplot(confusionMatrixNET$table)
```


