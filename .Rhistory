summary(model.gbm)
model.predictGBM = predict(model.gbm, test, type="response")
out_pred_numGBM <- ifelse(model.predictGBM > 0.5, 1, 0)
#out_predGBM <- factor(out_pred_numGBM, levels=c(0, 1))
out_predGBM<-as.factor(out_pred_numGBM)
levels(out_predGBM)=c("F","V")
confusionMatrixGBM<-confusionMatrix(data = out_predGBM, reference = test$outcome,positive="F")
confusionMatrixGBM
fourfoldplot(confusionMatrixGBM$table)
ControlParamteres <- trainControl(method = "cv",
number = 5, #usamos 5 fold cross-validation
savePredictions = TRUE,
classProbs = TRUE #give the probabilities for each class.Not just the class labels
)
parametersGrid <-  expand.grid(eta = 0.1,
colsample_bytree=c(0.5,0.7),
max_depth=c(3,6),
nrounds=100,
gamma=1,
min_child_weight=2,
subsample = 1
)
modelxgboost <- train(outcome~.,
data = train,
method = "xgbTree",
trControl = ControlParamteres,
tuneGrid=parametersGrid)
modelxgboost
predictions<-predict(modelxgboost,test)
#out_predGBOOST<-as.factor(ifelse(predictions > 0.5, 1, 0))
#levels(out_predGBOOST)=c("F","V")
confusionMatrixGBOOST<-confusionMatrix(data = predictions, reference = test$outcome,positive="F")
confusionMatrixGBOOST
fourfoldplot(confusionMatrixGBOOST$table)
#Max-Min Normalization
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
trainNorm<-train
testNorm<-test
trainNorm$outcome<-as.numeric(trainNorm$outcome)
testNorm$outcome<-as.numeric(testNorm$outcome)
trainNorm <- as.data.frame(lapply(trainNorm, normalize))
testNorm <- as.data.frame(lapply(testNorm, normalize))
trainNorm$outcome<-train$outcome
testNorm$outcome<-test$outcome
set.seed(42)
model.nn = nnet(outcome ~ ., data = trainNorm, size = 2, rang = 0.1, maxit = 200)
print(model.nn)
plotnet(model.nn, alpha=0.6)
model.predictNET = predict(model.nn, testNorm)
#Si usamos factores
out_pred_numNET<-as.factor(ifelse(model.predictNET > 0.5, 1, 0))
levels(out_pred_numNET)=c("F","V")
confusionMatrixNET<-confusionMatrix(data=out_pred_numNET,reference=testNorm$outcome,positive="F")
confusionMatrixNET
fourfoldplot(confusionMatrixNET$table)
library(nnet)
ctrl <- trainControl(method = "repeatedcv", number = 10, repeats=10)
model <- train(outcome~., data = cadets,
method = "nnet", trControl = ctrl,
linout = TRUE, preProcess=c("scale","center"),
na.action = na.omit)
library(nnet)
ctrl <- trainControl(method = "repeatedcv", number = 10, repeats=10)
model <- train(outcome~., data = train,
method = "nnet", trControl = ctrl,
linout = TRUE, preProcess=c("scale","center"),
na.action = na.omit)
rm(list = ls())
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
final <- as.data.frame(lapply(final, normalize))
summary(final)
final$outcome<-as.factor(final$outcome)
ind<-sample.split(Y=final$outcome,SplitRatio =0.7)
train <- final[ind,]
test <- final[!ind,]
final <- read.csv("C:/Users/Abel de Andrés Gómez/OneDrive/TFM/TraumaticData.csv",na.strings=c("","NA"))
summary(final)
#Max-Min Normalization
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
final <- as.data.frame(lapply(final, normalize))
summary(final)
final$outcome<-as.factor(final$outcome)
ind<-sample.split(Y=final$outcome,SplitRatio =0.7)
train <- final[ind,]
test <- final[!ind,]
library(rpart)
library(caret)
library(rattle)
library(RColorBrewer)
library(randomForest)
library(ROCR)
library(caTools)
library(gbm)
rm(list = ls())
set.seed(42)
final <- read.csv("C:/Users/Abel de Andrés Gómez/OneDrive/TFM/TraumaticData.csv",na.strings=c("","NA"))
summary(final)
#Max-Min Normalization
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
final <- as.data.frame(lapply(final, normalize))
summary(final)
final$outcome<-as.factor(final$outcome)
ind<-sample.split(Y=final$outcome,SplitRatio =0.7)
train <- final[ind,]
test <- final[!ind,]
ctrl <- trainControl(method = "repeatedcv", number = 10, repeats=10)
model <- train(outcome~., data = train,
method = "nnet", trControl = ctrl,
linout = TRUE, preProcess=c("scale","center"),
na.action = na.omit)
warnings()
ctrl <- trainControl(method = "cv", number = 10, savePredictions = TRUE,
classProbs = TRUE #give the probabilities for each class.Not just the class labels
)
model <- train(outcome~., data = train,
method = "nnet", trControl = ctrl,
linout = TRUE, preProcess=c("scale","center"),
na.action = na.omit)
rm(list = ls())
set.seed(42)
final <- read.csv("C:/Users/Abel de Andrés Gómez/OneDrive/TFM/TraumaticData.csv",na.strings=c("","NA"))
summary(final)
#Max-Min Normalization
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
final <- as.data.frame(lapply(final, normalize))
summary(final)
final$outcome<-as.factor(ifelse(final$outcome == "1", "F", "V"))
ind<-sample.split(Y=final$outcome,SplitRatio =0.7)
train <- final[ind,]
test <- final[!ind,]
ctrl <- trainControl(method = "cv", number = 10, savePredictions = TRUE,
classProbs = TRUE #give the probabilities for each class.Not just the class labels
)
model <- train(outcome~., data = train,
method = "nnet", trControl = ctrl,
linout = TRUE, preProcess=c("scale","center"),
na.action = na.omit)
warnings()
ctrl <- trainControl(method = "cv", number = 10, savePredictions = TRUE,
classProbs = TRUE #give the probabilities for each class.Not just the class labels
)
model <- train(outcome~., data = train,
method = "nnet", trControl = ctrl,
linout = FALSE, preProcess=c("scale","center"),
na.action = na.omit)
NNPredictions <-predict(model, test)
cmNN <-confusionMatrix(NNPredictions, testData$Class)
cmNN <-confusionMatrix(NNPredictions, test$outcome)
print(cmNN)
ctrl <- trainControl(method = "cv", number = 10, repeats=10,savePredictions = TRUE,
classProbs = TRUE #give the probabilities for each class.Not just the class labels
)
model <- train(outcome~., data = train,
method = "nnet", trControl = ctrl,
linout = FALSE, preProcess=c("scale","center"),
na.action = na.omit)
NNPredictions <-predict(model, test)
cmNN <-confusionMatrix(NNPredictions, test$outcome)
print(cmNN)
knitr::opts_chunk$set(echo = TRUE)
options(Encoding="UTF-8")
library(caTools)
library(caret)
library(randomForest)
library(nnet)
library(neuralnet)
library(party) #ctree
library(gbm)
library(rfUtilities)
library(NeuralNetTools)
library(e1071)
library(adabag) #adaboost
rm(list = ls())
final <- read.csv("C:/Users/Abel de Andrés Gómez/OneDrive/TFM/TraumaticData.csv",na.strings=c("","NA"))
final$outcome<-as.factor(ifelse(final$outcome == "1", "F", "V"))
#Cambiamos la variable outcome a factor para trabajar con el modelo
final$outcome<-as.factor(final$outcome)
set.seed(42)
ind<-sample.split(Y=final$outcome,SplitRatio =0.7)
train <- final[ind,]
test <- final[!ind,]
#Max-Min Normalization
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
trainNorm<-train
testNorm<-test
trainNorm$outcome<-as.numeric(trainNorm$outcome)
testNorm$outcome<-as.numeric(testNorm$outcome)
trainNorm <- as.data.frame(lapply(trainNorm, normalize))
testNorm <- as.data.frame(lapply(testNorm, normalize))
trainNorm$outcome<-train$outcome
testNorm$outcome<-test$outcome
#Max-Min Normalization
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
trainNorm<-train
testNorm<-test
trainNorm$outcome<-as.numeric(trainNorm$outcome)
testNorm$outcome<-as.numeric(testNorm$outcome)
trainNorm <- as.data.frame(lapply(trainNorm, normalize))
testNorm <- as.data.frame(lapply(testNorm, normalize))
trainNorm$outcome<-train$outcome
testNorm$outcome<-test$outcome
set.seed(42)
ctrl <- trainControl(method = "cv", number = 10, repeats=10,savePredictions = TRUE,
classProbs = TRUE #give the probabilities for each class.Not just the class labels
)
model.nn <- train(outcome~., data = trainNorm,
method = "nnet", trControl = ctrl,
linout = FALSE, preProcess=c("scale","center"),
na.action = na.omit)
print(model.nn)
plotnet(model.nn, alpha=0.6)
NNPredictions <-predict(model.nn, testNorm)
confusionMatrixNET<-confusionMatrix(data=NNPredictions,reference=testNorm$outcome,positive="F")
confusionMatrixNET
fourfoldplot(confusionMatrixNET$table)
# calculate correlation matrix
correlationMatrix <- cor(final)
# calculate correlation matrix
correlationMatrix <- cor(final[,10:13])
# find attributes that are highly corrected (ideally >0.75)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.75)
highlyCorrelated
MCOR <- cor(final)
MCOR <- cor(final[,1:13])
col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
cex.before <- par("cex")
par(cex = 0.5)
corrplot(MCOR, method="color", col=col(200),
type="upper", order="hclust",
addCoef.col = "black", # Add coefficient of correlation
tl.col="black", tl.srt=45, tl.cex = 1/par("cex"), #Text label color and rotation
# hide correlation coefficient on the principal diagonal
diag=FALSE
)
install.packages("corrplot")
library("corrplot", lib.loc="~/R/win-library/3.5")
MCOR <- cor(final[,1:13])
col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
cex.before <- par("cex")
par(cex = 0.5)
corrplot(MCOR, method="color", col=col(200),
type="upper", order="hclust",
addCoef.col = "black", # Add coefficient of correlation
tl.col="black", tl.srt=45, tl.cex = 1/par("cex"), #Text label color and rotation
# hide correlation coefficient on the principal diagonal
diag=FALSE
)
calculate correlation matrix
# calculate correlation matrix
correlationMatrix <- cor(education[,1:13])
# calculate correlation matrix
correlationMatrix <- cor(final[,1:13])
# find attributes that are highly corrected (ideally >0.75)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.75)
# calculate correlation matrix
correlationMatrix <- cor(final[,1:13])
# find attributes that are highly corrected (ideally >0.75)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.75)
highlyCorrelated
# calculate correlation matrix
correlationMatrix <- cor(final[,1:13])
# find attributes that are highly corrected (ideally >0.75)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.50)
highlyCorrelated
MCOR <- cor(final[,1:13])
col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
cex.before <- par("cex")
par(cex = 0.5)
corrplot(MCOR, method="color", col=col(200),
type="upper", order="hclust",
addCoef.col = "black", # Add coefficient of correlation
tl.col="black", tl.srt=45, tl.cex = 1/par("cex"), #Text label color and rotation
# hide correlation coefficient on the principal diagonal
diag=FALSE
)
# calculate correlation matrix
correlationMatrix <- cor(final[,1:13])
# find attributes that are highly corrected (ideally >0.75)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.50)
highlyCorrelated
set.seed(42)
trainNormCor <-trainNorm[,-7]
testNormCor <-testNorm[,-7]
ctrl <- trainControl(method = "cv", number = 10 ,savePredictions = TRUE,
classProbs = TRUE #give the probabilities for each class.Not just the class labels
)
model.nn <- train(outcome~., data = trainNormCor,
method = "nnet", trControl = ctrl,
linout = FALSE, preProcess=c("scale","center"),
na.action = na.omit)
knitr::opts_chunk$set(echo = TRUE)
options(Encoding="UTF-8")
library(caTools)
library(caret)
library(randomForest)
library(nnet)
library(neuralnet)
library(party) #ctree
library(gbm)
library(rfUtilities)
library(NeuralNetTools)
library(e1071)
library(adabag) #adaboost
library(corrplot)
rm(list = ls())
final <- read.csv("C:/Users/Abel de Andrés Gómez/OneDrive/TFM/TraumaticData.csv",na.strings=c("","NA"))
final$outcome<-as.factor(ifelse(final$outcome == "1", "F", "V"))
#Cambiamos la variable outcome a factor para trabajar con el modelo
final$outcome<-as.factor(final$outcome)
set.seed(42)
ind<-sample.split(Y=final$outcome,SplitRatio =0.7)
train <- final[ind,]
test <- final[!ind,]
set.seed(42)
modelo.logit <- glm(outcome ~ .,
data = train, family = "binomial")
summary(modelo.logit)
options(warn=-1)
pred <- predict(modelo.logit, newdata = test, type = "response")  # predicted probabilities
options(warn=1)
out_pred_num <- ifelse(pred > 0.5, 1, 0)
#out_pred <- factor(out_pred_num, levels=c(0, 1))
out_pred<-as.factor(out_pred_num)
levels(out_pred)=c("F","V")
confusionMatrix<-confusionMatrix(data = out_pred, reference = test$outcome,positive="F")
confusionMatrix
fourfoldplot(confusionMatrix$table)
ctrl <- trainControl(method = "repeatedcv", number = 10, savePredictions = TRUE)
mod_fit <- train(outcome ~ .,data = train, method="glm", family="binomial",
trControl = ctrl, tuneLength = 5)
predCV = predict(mod_fit, newdata=test)
confusionMatrixCV<-confusionMatrix(data = predCV, reference = test$outcome,positive="F")
confusionMatrixCV
fourfoldplot(confusionMatrix$table)
set.seed(42)
modelo.logit2 <- glm(outcome ~ age+ec+eye+motor+verbal+pupils+phm+sah+oblt+mdls+hmt,
data = train, family = "binomial")
summary(modelo.logit2)
options(warn=-1)
pred2 <- predict(modelo.logit2, newdata = test, type = "response")  # predicted probabilities
options(warn=1)
out_pred_num2 <- ifelse(pred2 > 0.5, 1, 0)
#out_pred2 <- factor(out_pred_num2, levels=c(0, 1))
out_pred2<-as.factor(out_pred_num2)
levels(out_pred2)=c("F","V")
confusionMatrix2<-confusionMatrix(data = out_pred2, reference = test$outcome,positive="F")
confusionMatrix2
fourfoldplot(confusionMatrix2$table)
set.seed(42)
model.RF=randomForest(outcome~.,data=train,ntree=500)
print(model.RF)
train.RF<-train
train.RF$outcome<-as.numeric(train.RF$outcome)
mtry=tuneRF(x = train.RF,       # data set de entrenamiento
y = train.RF$outcome,  # variable a predecir
mtryStart  = 1,   # cantidad de variables inicial
stepFactor = 2,   # incremento de variables
ntreeTry   = 500, # cantidad arboles a ejecutar en cada iteracion
improve    = .01  # mejora minina del OOB para seguir iteraciones
)
best.m <- mtry[mtry[, 2] == min(mtry[, 2]), 1]
print(mtry)
print(best.m)
model.RF2=randomForest(outcome~.,data=final,importance=T,ntree=500, mtry=best.m)
print(model.RF)
predRF <- predict(model.RF2, newdata = test)
confusionMatrixRF<-confusionMatrix(data=predRF,reference=test$outcome,positive="F")
confusionMatrixRF
fourfoldplot(confusionMatrixRF$table)
set.seed(42)
train.ctree=ctree(outcome ~ age+ec+eye+motor+verbal+pupils+phm+sah+oblt+mdls+hmt+sex+cause, data=train)
#train.ctree
#plot(train.ctree, main="Árbol de decisión")
ctree.predict=predict(train.ctree,test)
confusionMatrixCTREE<-confusionMatrix(data=ctree.predict,reference=test$outcome,positive="F")
confusionMatrixCTREE
fourfoldplot(confusionMatrixCTREE$table)
set.seed(42)
adaboost<-boosting(outcome ~ ., data=train, boos=TRUE, mfinal=20,coeflearn='Breiman')
#summary(adaboost)
adaboost.predict=predict(adaboost,test)
confusionMatrixAdaBoost<-confusionMatrix(data=as.factor(adaboost.predict$class),reference=test$outcome,positive="F")
confusionMatrixAdaBoost
fourfoldplot(confusionMatrixAdaBoost$table)
#https://rpubs.com/omicsdata/gbm
trainGBM<-train
trainGBM$outcome<-as.numeric(trainGBM$outcome)
trainGBM = transform(trainGBM, outcome=outcome-1)
set.seed(42)
model.gbm = gbm(outcome ~ ., data = trainGBM, shrinkage=0.01, distribution = 'bernoulli', cv.folds=5, n.trees=3000, verbose=F)
model.gbm
best.iter.oob <- gbm.perf(model.gbm,method="OOB")  # returns out-of-bag estimated best number of trees
print(best.iter.oob)
best.iter.cv <- gbm.perf(model.gbm,method="cv")   # returns 5-fold cv estimate of best number of trees
print(best.iter.cv)
set.seed(42)
model.gbm = gbm(outcome ~ ., data = trainGBM, shrinkage=0.01, distribution = 'bernoulli', cv.folds=5, n.trees=best.iter.oob, verbose=F)
model.gbm
summary(model.gbm)
model.predictGBM = predict(model.gbm, test, type="response")
out_pred_numGBM <- ifelse(model.predictGBM > 0.5, 1, 0)
#out_predGBM <- factor(out_pred_numGBM, levels=c(0, 1))
out_predGBM<-as.factor(out_pred_numGBM)
levels(out_predGBM)=c("F","V")
confusionMatrixGBM<-confusionMatrix(data = out_predGBM, reference = test$outcome,positive="F")
confusionMatrixGBM
fourfoldplot(confusionMatrixGBM$table)
ControlParamteres <- trainControl(method = "cv",
number = 5, #usamos 5 fold cross-validation
savePredictions = TRUE,
classProbs = TRUE #give the probabilities for each class.Not just the class labels
)
parametersGrid <-  expand.grid(eta = 0.1,
colsample_bytree=c(0.5,0.7),
max_depth=c(3,6),
nrounds=100,
gamma=1,
min_child_weight=2,
subsample = 1
)
modelxgboost <- train(outcome~.,
data = train,
method = "xgbTree",
trControl = ControlParamteres,
tuneGrid=parametersGrid)
modelxgboost
predictions<-predict(modelxgboost,test)
#out_predGBOOST<-as.factor(ifelse(predictions > 0.5, 1, 0))
#levels(out_predGBOOST)=c("F","V")
confusionMatrixGBOOST<-confusionMatrix(data = predictions, reference = test$outcome,positive="F")
confusionMatrixGBOOST
fourfoldplot(confusionMatrixGBOOST$table)
#Max-Min Normalization
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
trainNorm<-train
testNorm<-test
trainNorm$outcome<-as.numeric(trainNorm$outcome)
testNorm$outcome<-as.numeric(testNorm$outcome)
trainNorm <- as.data.frame(lapply(trainNorm, normalize))
testNorm <- as.data.frame(lapply(testNorm, normalize))
trainNorm$outcome<-train$outcome
testNorm$outcome<-test$outcome
MCOR <- cor(final[,1:13])
col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
cex.before <- par("cex")
par(cex = 0.5)
corrplot(MCOR, method="color", col=col(200),
type="upper", order="hclust",
addCoef.col = "black", # Add coefficient of correlation
tl.col="black", tl.srt=45, tl.cex = 1/par("cex"), #Text label color and rotation
# hide correlation coefficient on the principal diagonal
diag=FALSE
)
# calculate correlation matrix
correlationMatrix <- cor(final[,1:13])
# find attributes that are highly corrected (ideally >0.75)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.50)
highlyCorrelated
set.seed(42)
ctrl <- trainControl(method = "cv", number = 10 ,savePredictions = TRUE,
classProbs = TRUE #give the probabilities for each class.Not just the class labels
)
model.nn <- train(outcome~., data = trainNorm,
method = "nnet", trControl = ctrl,
linout = FALSE, preProcess=c("scale","center"),
na.action = na.omit
, tuneGrid=expand.grid(.size=c(1,5,10),.decay=c(0,0.001,0.1)))
#print(model.nn)
plotnet(model.nn, alpha=0.6)
NNPredictions <-predict(model.nn, testNorm)
confusionMatrixNET<-confusionMatrix(data=NNPredictions,reference=testNorm$outcome,positive="F")
confusionMatrixNET
fourfoldplot(confusionMatrixNET$table)
set.seed(42)
trainNormCor <-trainNorm[,-7]
testNormCor <-testNorm[,-7]
ctrl <- trainControl(method = "cv", number = 10 ,savePredictions = TRUE,
classProbs = TRUE #give the probabilities for each class.Not just the class labels
)
model.nn <- train(outcome~., data = trainNormCor,
method = "nnet", trControl = ctrl,
linout = FALSE, preProcess=c("scale","center"),
na.action = na.omit)
#print(model.nn)
NNPredictions <-predict(model.nn, testNorm)
confusionMatrixNET<-confusionMatrix(data=NNPredictions,reference=testNorm$outcome,positive="F")
confusionMatrixNET
set.seed(42)
ctrl <- trainControl(method = "cv", number = 10 ,savePredictions = TRUE,
classProbs = TRUE #give the probabilities for each class.Not just the class labels
)
model.nn <- train(outcome~., data = trainNorm,
method = "nnet", trControl = ctrl,
linout = FALSE, preProcess=c("scale","center"),
na.action = na.omit
, tuneGrid=expand.grid(.size=c(1,5,10),.decay=c(0,0.001,0.1)))
